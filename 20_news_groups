import nltk
nltk.download()
from sklearn.datasets import fetch_20newsgroups
groups=fetch_20newsgroups()
groups.keys()
groups['target_names']
groups.target
groups.data[0]
len(groups.data[0])
len(groups.data[1])
import seaborn as sns
sns.distplot(groups.target)

import matplotlib.pyplot as plt
plt.show()
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_20newsgroups

cv=CountVectorizer(stop_words="english",max_features=500)
groups=fetch_20newsgroups()
transformed=cv.fit_transform(groups.data)
print(cv.get_feature_names())

sns.distplot(np.log(transformed.toarray().sum(axis=0)))
plt.xlabel('logcount')
plt.ylabel('frequency')
plt.title('plot of 500 word count')
plt.show()
print(cv.get_feature_names())
----------------------------------------------
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.datasets import fetch_20newsgroups
from nltk.corpus import names
from nltk.stem import WordNetLemmatizer
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

def letters_only(astr):
    return astr.isalpha()


cv=CountVectorizer(stop_words="english",max_features=500)
groups=fetch_20newsgroups()
cleaned=[]
all_names=set(names.words())
lemmatizer=WordNetLemmatizer()

for post in groups.data:
    cleaned.append(''.join([
                        lemmatizer.lemmatize(word.lower())
                        for word in post.split()
                        if letters_only(word)
                        and word not in all_names]))
        
transformed=cv.fit_transform(cleaned)
km=KMeans(n_clusters=20)
km.fit(transformed)
labels=groups.target
plt.scatter(labels,km.labels_)
plt.xlabel('Newsgroup')
plt.ylabel('Cluster')
plt.show()
----------------------------------------
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.datasets import fetch_20newsgroups
from nltk.corpus import names
from nltk.stem import WordNetLemmatizer
from sklearn.decomposition import NMF

def letters_only(astr):
    return astr.isalpha()

cv=CountVectorizer(stop_words="english",max_features=500)
groups=fetch_20newsgroups()
cleaned=[]
all_names=set(names.words())
lemmatizer=WordNetLemmatizer()

for post in groups.data:
    cleaned.append(''.join([ 
                        lemmatizer.lemmatize(word.lower())
                        for word in post.split()
                        if letters_only(word)
                        and word not in all_names]))
        
transformed=cv.fit_transform(cleaned)
nmf = NMF(n_components=100, random_state=43).fit(transformed)
for topic_idx, topic in enumerate(nmf.components_):
    label='{}:'.format(topic_idx)
    print(label, " ".join([cv.get_feature_names()[i]
                        for i in topic.argsort()[:-9:-1]]))
